{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Llama for SUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q -U accelerate==0.23.0 peft==0.5.0 bitsandbytes==0.41.1 transformers==4.31 trl==0.7.2\n",
    "!pip3 install xformers\n",
    "!pip3 install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file into a pandas DataFrame\n",
    "\n",
    "filename = \"/data/ARENAS_Automatic_Extremist_Analysis/ARENAS_Automatic_Extremist_Analysis/Data/SUD_data/all.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "# Rename columnns\n",
    "df.rename(columns={'text': 'input', 'class': 'labels'}, inplace=True)\n",
    "# Select only the \"input\" and \"labels\" columns from the DataFrame\n",
    "df = df[[\"input\", \"labels\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as a woman you shouldnt complain about cleanin...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boy dats cold tyga dwn bad for cuffin dat hoe ...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dawg you ever fuck a bitch and she sta to cry ...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she look like a tranny</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the shit you hear about me might be true or it...</td>\n",
       "      <td>offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470763</th>\n",
       "      <td>they belong to you flight diy terrorist countr...</td>\n",
       "      <td>aggressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470764</th>\n",
       "      <td>really motivating programme congratulations to...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470765</th>\n",
       "      <td>fabricated news</td>\n",
       "      <td>aggressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470766</th>\n",
       "      <td>whats wrong with you secular idiots</td>\n",
       "      <td>aggressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470767</th>\n",
       "      <td>looks like inevitable after all political hard...</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input      labels\n",
       "0       as a woman you shouldnt complain about cleanin...     neither\n",
       "1       boy dats cold tyga dwn bad for cuffin dat hoe ...   offensive\n",
       "2       dawg you ever fuck a bitch and she sta to cry ...   offensive\n",
       "3                                  she look like a tranny   offensive\n",
       "4       the shit you hear about me might be true or it...   offensive\n",
       "...                                                   ...         ...\n",
       "470763  they belong to you flight diy terrorist countr...  aggressive\n",
       "470764  really motivating programme congratulations to...     neither\n",
       "470765                                    fabricated news  aggressive\n",
       "470766                whats wrong with you secular idiots  aggressive\n",
       "470767  looks like inevitable after all political hard...     neither\n",
       "\n",
       "[470768 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neither' 'offensive' 'hate' 'abusive' 'profane' 'severe_toxic' 'toxic'\n",
      " 'identity_hate' 'insult' 'obscene' 'threat' 'aggressive']\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and 'column_name' is the name of the column\n",
    "unique_values = df['labels'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'column_name' is the name of the column\n",
    "df['labels'] = df['labels'].replace({'severe_toxic': 'severe', 'identity_hate': 'identity', \"insult\": \"ins\", \"offensive\": \"off\", \"profane\": \"prof\", \"obscene\": \"obsc\", \"toxic\": \"to\"})\n",
    "\n",
    "# Now 'severe_toxic' should be replaced with 'severe toxic' and 'intentity_hate' with 'identity hate'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neither' 'off' 'hate' 'abusive' 'prof' 'severe' 'to' 'identity' 'ins'\n",
      " 'obsc' 'threat' 'aggressive']\n"
     ]
    }
   ],
   "source": [
    "unique_values = df['labels'].unique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting and Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list()\n",
    "X_test = list()\n",
    "# Split the data into training, testing, and evaluation sets for each sentiment label\n",
    "for labels in [\"off\", \"neither\", \"hate\", \"severe\", \"to\", \"ins\", \"prof\", \"obsc\", \"identity\", \"threat\"]:\n",
    "    train, test  = train_test_split(df[df.labels==labels], \n",
    "                                    train_size=344,\n",
    "                                    test_size=344, \n",
    "                                    random_state=42)\n",
    "    X_train.append(train)\n",
    "    X_test.append(test)\n",
    "\n",
    "# Concatenate and shuffle the training and testing sets\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "X_test = pd.concat(X_test)\n",
    "\n",
    "# Select evaluation samples from the remaining data\n",
    "eval_idx = [idx for idx in df.index if idx not in list(train.index) + list(test.index)]\n",
    "X_eval = df[df.index.isin(eval_idx)]\n",
    "X_eval = (X_eval\n",
    "          .groupby('labels', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "\n",
    "# Reset index of the training set\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "# Function to generate prompts for training and evaluation data\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Categorize the tweet enclosed in square brackets to determine if it is off, or neither, or hate, or severe, or to, or ins, or prof, or obsc, or identity, or threat, \n",
    "            and return the answer as the corresponding label:\n",
    "            \"off\" or \"neither\" or \"hate\" or \"severe\" or \"to\" or \"ins\" or \"prof\" or \"obsc\" or \"identity\" or \"threat\". \n",
    "            Make sure to give the whole label as an answer.\n",
    "            [{data_point[\"input\"]}] = {data_point[\"labels\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "# Function to generate prompts for testing data\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Categorize the tweet enclosed in square brackets to determine if it is off, or neither, or hate, or severe, or to, or ins, or prof, or obsc, or identity, or threat, \n",
    "            and return the answer as the corresponding label:\n",
    "            \"off\" or \"neither\" or \"hate\" or \"severe\" or \"to\" or \"ins\" or \"prof\" or \"obsc\" or \"identity\" or \"threat\".\n",
    "            Make sure to give the whole label as an answer.\n",
    "            [{data_point[\"input\"]}] = \"\"\".strip()\n",
    "\n",
    "# Convert the prompts into DataFrames for training, evaluation, and testing\n",
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"input\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"input\"])\n",
    "\n",
    "y_true = X_test.labels\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"input\"])\n",
    "\n",
    "# Create datasets from the generated prompts\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'threat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    # Define labels and their corresponding numeric mappings\n",
    "    labels = [\"off\", \"neither\", \"hate\", \"severe\", \"to\", \"ins\", \"prof\", \"obsc\", \"identity\", \"threat\"]\n",
    "    mapping = {\"off\": 9, \"neither\": 8, \"hate\": 7, \"severe\" : 6, \"to\": 5, \"ins\": 4, \"prof\": 3, \"obsc\": 2, \"identity\": 1, \"threat\": 0}\n",
    "    \n",
    "    # Function to map labels to numeric values\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    # Apply mapping to true and predicted labels\n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:07<00:00, 63.93s/it]\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"../input/llama-2/pytorch/7b-hf/1\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "# Define the data type for model computation\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Configure BitsAndBytes quantization parameters\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Initialize the model with quantization settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "# Initialize the model with quantization settings\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Initialize the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True,\n",
    "                                         )\n",
    "# Set padding token and side for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Make predictions using a text generation model.\n",
    "\n",
    "    Parameters:\n",
    "        test (pd.DataFrame): DataFrame containing test data.\n",
    "        model: Pre-trained text generation model.\n",
    "        tokenizer: Tokenizer for the model.\n",
    "\n",
    "    Returns:\n",
    "        list: Predicted labels for each input.\n",
    "    \"\"\"    \n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"input\"]\n",
    "        # Create a text generation pipeline\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 1, \n",
    "                        temperature = 0.0,\n",
    "                       )\n",
    "        # Generate text based on the prompt\n",
    "        result = pipe(prompt)\n",
    "        #print(result)\n",
    "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "        #print(answer)\n",
    "        # Map the generated text to sentiment labels\n",
    "        if \"off\" in answer:\n",
    "            y_pred.append(\"off\")\n",
    "        elif \"hate\" in answer:\n",
    "            y_pred.append(\"hate\")\n",
    "        elif \"severe\" in answer:\n",
    "            y_pred.append(\"severe\")\n",
    "        elif \"to\" in answer:\n",
    "            y_pred.append(\"to\")\n",
    "        elif \"ins\" in answer:\n",
    "            y_pred.append(\"ins\")\n",
    "        elif \"prof\" in answer:\n",
    "            y_pred.append(\"prof\")\n",
    "        elif \"obsc\" in answer:\n",
    "            y_pred.append(\"obsc\")\n",
    "        elif \"identity\" in answer:\n",
    "            y_pred.append(\"identity\")\n",
    "        elif \"threat\" in answer:\n",
    "            y_pred.append(\"threat\")\n",
    "        else:\n",
    "            y_pred.append(\"neither\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(test, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.100\n",
      "Accuracy for label 0: 0.000\n",
      "Accuracy for label 1: 0.000\n",
      "Accuracy for label 2: 0.000\n",
      "Accuracy for label 3: 0.000\n",
      "Accuracy for label 4: 0.000\n",
      "Accuracy for label 5: 0.000\n",
      "Accuracy for label 6: 0.000\n",
      "Accuracy for label 7: 0.000\n",
      "Accuracy for label 8: 1.000\n",
      "Accuracy for label 9: 0.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00        10\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.00      0.00      0.00        10\n",
      "           7       0.00      0.00      0.00        10\n",
      "           8       0.10      1.00      0.18        10\n",
      "           9       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.10       100\n",
      "   macro avg       0.01      0.10      0.02       100\n",
      "weighted avg       0.01      0.10      0.02       100\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  1]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3440/3440 [00:00<00:00, 4053.47 examples/s]\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 4162.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# LoRA Model Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Training Arguments Configuration\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"logs\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8, # 4\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer Initialization\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"input\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 1:34:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.814600</td>\n",
       "      <td>0.716811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.711314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.633100</td>\n",
       "      <td>0.711217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.715227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3440/3440 [13:48<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.559\n",
      "Accuracy for label 0: 0.797\n",
      "Accuracy for label 1: 0.590\n",
      "Accuracy for label 2: 0.500\n",
      "Accuracy for label 3: 0.581\n",
      "Accuracy for label 4: 0.244\n",
      "Accuracy for label 5: 0.375\n",
      "Accuracy for label 6: 0.494\n",
      "Accuracy for label 7: 0.602\n",
      "Accuracy for label 8: 0.738\n",
      "Accuracy for label 9: 0.666\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77       344\n",
      "           1       0.53      0.59      0.56       344\n",
      "           2       0.45      0.50      0.47       344\n",
      "           3       0.69      0.58      0.63       344\n",
      "           4       0.44      0.24      0.31       344\n",
      "           5       0.42      0.38      0.40       344\n",
      "           6       0.55      0.49      0.52       344\n",
      "           7       0.45      0.60      0.51       344\n",
      "           8       0.70      0.74      0.72       344\n",
      "           9       0.59      0.67      0.63       344\n",
      "\n",
      "    accuracy                           0.56      3440\n",
      "   macro avg       0.56      0.56      0.55      3440\n",
      "weighted avg       0.56      0.56      0.55      3440\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[274  17   2   1   1   7  30   5   5   2]\n",
      " [ 10 203  15   1   9  12  29  49   5  11]\n",
      " [  6   8 172  29  33  42  21  14   7  12]\n",
      " [  2   1  24 200   7   4   5  30  23  48]\n",
      " [  8  29  54  11  84  64  49  27   3  15]\n",
      " [ 19  25  65   4  34 129   0  26  32  10]\n",
      " [ 39  47  30   1  14  19 170  14   3   7]\n",
      " [  1  45   5  16   3   4   5 207  22  36]\n",
      " [  2   2   6  10   2  25   0  26 254  17]\n",
      " [  6   4   8  16   3   2   1  66   9 229]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate       0.45      0.60      0.51       344\n",
      "    identity       0.53      0.59      0.56       344\n",
      "         ins       0.44      0.24      0.31       344\n",
      "     neither       0.70      0.74      0.72       344\n",
      "        obsc       0.45      0.50      0.47       344\n",
      "         off       0.59      0.67      0.63       344\n",
      "        prof       0.69      0.58      0.63       344\n",
      "      severe       0.55      0.49      0.52       344\n",
      "      threat       0.75      0.80      0.77       344\n",
      "          to       0.42      0.38      0.40       344\n",
      "\n",
      "    accuracy                           0.56      3440\n",
      "   macro avg       0.56      0.56      0.55      3440\n",
      "weighted avg       0.56      0.56      0.55      3440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def log_training_results(model_name, bnb_config, peft_config, training_arguments, trainer_state, splitting_info, prompt_generation_info, y_true, y_pred):\n",
    "    log_file = \"training_log.txt\"\n",
    "\n",
    "    # Calculate classification report and confusion matrix\n",
    "    classification_rep = classification_report(y_true, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred).tolist()\n",
    "\n",
    "    # Append the current training information to the log file\n",
    "    with open(log_file, 'a') as txtfile:\n",
    "        txtfile.write(f\"Model Name: {model_name}\\n\")\n",
    "        txtfile.write(f\"BitsAndBytes Config: {str(bnb_config)}\\n\")\n",
    "        txtfile.write(f\"Lora Config: {str(peft_config)}\\n\")\n",
    "        txtfile.write(f\"Training Arguments: {str(training_arguments)}\\n\")\n",
    "        txtfile.write(f\"Splitting Info: {splitting_info}\\n\")\n",
    "        txtfile.write(f\"Prompt Generation Info: {prompt_generation_info}\\n\")\n",
    "        txtfile.write(f\"Classification Report:\\n{classification_rep}\\n\")\n",
    "        txtfile.write(f\"Confusion Matrix:\\n{confusion_mat}\\n\\n\")\n",
    "\n",
    "    # Return the classification report\n",
    "    return classification_rep\n",
    "\n",
    "\n",
    "# Log training parameters, results, splitting info, prompt generation info, and prediction results\n",
    "splitting_info = \"Training samples: {}, Testing samples: {}, Evaluation samples: {}\".format(len(X_train), len(X_test), len(X_eval))\n",
    "prompt_generation_info = \"Prompt generation details:  Categorize the tweet enclosed in square brackets to determine if it is off, or neither, or hate, or severe, or to, or ins, or prof, or obsc, or identity, or threat, and return the answer as the corresponding label: off or neither or hate or severe or to or ins or prof or obsc or identity or threat. Make sure to give the whole label as an answer.\"  # Add details about how prompts were generated\n",
    "\n",
    "# Store the classification report in a text file\n",
    "classification_rep = log_training_results(model_name, bnb_config, peft_config, training_arguments, trainer.state, splitting_info, prompt_generation_info, y_true, y_pred)\n",
    "\n",
    "# Now you can access the classification report\n",
    "print(classification_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
